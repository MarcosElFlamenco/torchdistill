# Contributions
1. Yoshitomo Matsubara, ["torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation"](https://arxiv.org/abs/2011.12913), Third Workshop on Reproducible Research in Pattern Recognition at ICPR 2020
2. Yoshitomo Matsubara, ["torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP"](https://aclanthology.org/2023.nlposs-1.18/), Third Workshop for NLP Open Source Software at EMNLP 2023
3. Roy Miles, Krystian Mikolajczyk, ["Understanding the Role of the Projector in Knowledge Distillation"](https://ojs.aaai.org/index.php/AAAI/article/view/28219), AAAI 2024
  
Your papers will be here if you provide a set of files to ensure the reproducibility e.g., yaml, log and checkpoint files.  
Send a pull request with the files and reference. :)
